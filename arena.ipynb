{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dataset was not found at: ML_replay_memories/random_random_10k_games.txt !",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mML_bot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_model\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/VS code/Project/schnapsen-main/src/schnapsen/schanpsen-bot/models/ML_bot.py:518\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model_type)\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_location\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m exists already and will be overwritten as selected.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    515\u001b[0m     )\n\u001b[1;32m    516\u001b[0m     model_location\u001b[38;5;241m.\u001b[39munlink()\n\u001b[0;32m--> 518\u001b[0m \u001b[43mtrain_ML_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreplay_memory_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplay_memory_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/VS code/Project/schnapsen-main/src/schnapsen/schanpsen-bot/models/ML_bot.py:153\u001b[0m, in \u001b[0;36mtrain_ML_model\u001b[0;34m(replay_memory_location, model_location, model_class)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# check that the replay memory dataset is found at the specified location\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m replay_memory_location\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset was not found at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreplay_memory_location\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m !\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# Check if model exists already\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_location\u001b[38;5;241m.\u001b[39mexists():\n",
      "\u001b[0;31mValueError\u001b[0m: Dataset was not found at: ML_replay_memories/random_random_10k_games.txt !"
     ]
    }
   ],
   "source": [
    "# ML_bot training\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"./models\")\n",
    "\n",
    "from ML_bot import train_model\n",
    "\n",
    "train_model(\"NN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roti/workspace/uni_projects/cardbot/models/Deepbot.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current progress: game 0\n",
      "DLBot: 373\n",
      "Randy: 518\n",
      "Rdeep: 609\n"
     ]
    }
   ],
   "source": [
    "# Round robin tournament, 3 bots\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"./models\")\n",
    "from ML_bot import MLPlayingBot\n",
    "from schnapsen.bots import RdeepBot\n",
    "from schnapsen.bots import RandBot\n",
    "from Deepbot import DeepLearningBot\n",
    "from schnapsen.game import SchnapsenGamePlayEngine\n",
    "import pathlib\n",
    "import random\n",
    "\n",
    "\n",
    "engine = SchnapsenGamePlayEngine()\n",
    "model_dir = \"ML_models\"\n",
    "model_name = \"simple_model\"\n",
    "model_location = pathlib.Path(model_dir) / model_name\n",
    "\n",
    "model_path = \"./models/model_20250112_044306_epochs25_batch128_lr0.004.pt\"\n",
    "\n",
    "rng = random.Random(42)\n",
    "\n",
    "bot1 = MLPlayingBot(model_location, name=\"MLBot\")\n",
    "bot2 = RandBot(rng, \"RandBot\")\n",
    "bot3 = RdeepBot(num_samples=5, depth=2, rand=rng, name=\"RdeepBot\")\n",
    "bot4 = DeepLearningBot(model_path=model_path, input_size=173, hidden_size=64, name=\"DLbot\")\n",
    "\n",
    "def round_robin_tournament():\n",
    "    wins_MLBot = 0\n",
    "    wins_RDeep = 0\n",
    "    wins_Randy = 0\n",
    "    for i in range (500):\n",
    "\n",
    "        if i + 1 % 500 == 0:\n",
    "            print(f\"Current progress: game {i}\")\n",
    "\n",
    "        winner_id, game_points, score = engine.play_game(bot1, bot4, random.Random(i))\n",
    "\n",
    "        if winner_id._Bot__name == \"DLbot\":\n",
    "            wins_MLBot +=1\n",
    "        elif winner_id._Bot__name == \"RdeepBot\":\n",
    "            wins_RDeep +=1\n",
    "        else:\n",
    "            wins_Randy +=1\n",
    "\n",
    "        winner_id2, game_points2, score2 = engine.play_game(bot1, bot3, random.Random(i))\n",
    "\n",
    "        if winner_id2._Bot__name == \"DLbot\":\n",
    "            wins_MLBot +=1\n",
    "        elif winner_id2._Bot__name == \"RdeepBot\":\n",
    "            wins_RDeep +=1\n",
    "        else:\n",
    "            wins_Randy +=1\n",
    "\n",
    "        winner_id3, game_points3, score3 = engine.play_game(bot4, bot3, random.Random(i))\n",
    "\n",
    "        if winner_id3._Bot__name == \"DLbot\":\n",
    "            wins_MLBot +=1\n",
    "        elif winner_id3._Bot__name == \"RdeepBot\":\n",
    "            wins_RDeep +=1\n",
    "        else:\n",
    "            wins_Randy +=1\n",
    "\n",
    "\n",
    "\n",
    "    print (f\"DLBot: {wins_MLBot}\")\n",
    "    print (f\"Randy: {wins_Randy}\")\n",
    "    print (f\"Rdeep: {wins_RDeep}\")\n",
    "\n",
    "round_robin_tournament()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_56315/3999041620.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  regret_net.load_state_dict(torch.load(\"./CFRmodels/regret_net.pth\"))\n",
      "/tmp/ipykernel_56315/3999041620.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  strategy_net.load_state_dict(torch.load(\"./CFRmodels/strategy_net.pth\"))\n",
      "/home/roti/workspace/uni_projects/cardbot/models/Deepbot.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLBot wins: 641\n",
      "DeepCFRBot wins: 359\n"
     ]
    }
   ],
   "source": [
    "# 1v1 arena\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"./models\")\n",
    "from ML_bot import MLPlayingBot\n",
    "from schnapsen.bots import RdeepBot\n",
    "from schnapsen.bots import RandBot\n",
    "from Deepbot import DeepLearningBot\n",
    "from schnapsen.game import SchnapsenGamePlayEngine\n",
    "import pathlib\n",
    "import random\n",
    "from DeepCFR import DeepCFRBot, RegretNetwork, StrategyNetwork\n",
    "import torch\n",
    "\n",
    "regret_net = RegretNetwork(input_size=173, action_size=1)  # Update action_size\n",
    "strategy_net = StrategyNetwork(input_size=173, action_size=1)\n",
    "\n",
    "regret_net.load_state_dict(torch.load(\"./CFRmodels/regret_net.pth\"))\n",
    "strategy_net.load_state_dict(torch.load(\"./CFRmodels/strategy_net.pth\"))\n",
    "\n",
    "regret_net.eval()\n",
    "strategy_net.eval()\n",
    "\n",
    "\n",
    "engine = SchnapsenGamePlayEngine()\n",
    "model_dir = \"ML_models\"\n",
    "model_name = \"simple_model\"\n",
    "model_location = pathlib.Path(model_dir) / model_name\n",
    "\n",
    "model_path = \"./models/model_20250112_044306_epochs25_batch128_lr0.004.pt\"\n",
    "\n",
    "rng = random.Random(42)\n",
    "\n",
    "bot1 = MLPlayingBot(model_location, name=\"MLBot\")\n",
    "bot2 = RandBot(rng, \"RandBot\")\n",
    "bot3 = RdeepBot(num_samples=10, depth=3, rand=rng, name=\"RdeepBot\")\n",
    "bot4 = DeepLearningBot(model_path=model_path, input_size=173, hidden_size=64, name=\"DLbot\")\n",
    "bot5 = RdeepBot(num_samples=20, depth=5, rand=rng, name=\"RdeepBot2\")\n",
    "bot6 = DeepCFRBot(regret_net, strategy_net, name=\"DeepCFRBot\")\n",
    "\n",
    "def matches_1v1(bot1, bot2):\n",
    "    wins_bot1 = 0\n",
    "    wins_bot2 = 0\n",
    "    \n",
    "    for i in range (1000):\n",
    "\n",
    "        if i + 1 % 500 == 0:\n",
    "            print(f\"Current progress: game {i}\")\n",
    "\n",
    "        winner_id, game_points, score = engine.play_game(bot1, bot2, random.Random(i))\n",
    "\n",
    "        if winner_id._Bot__name == str(bot1):\n",
    "            wins_bot1 +=1\n",
    "        elif winner_id._Bot__name == str(bot2):\n",
    "            wins_bot2 +=1\n",
    "\n",
    "\n",
    "    print (f\"{str(bot1)} wins: {wins_bot1}\")\n",
    "    print (f\"{str(bot2)} wins: {wins_bot2}\")\n",
    "\n",
    "matches_1v1(bot1, bot6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep learning training\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"./models\")\n",
    "from Deepbot import train_DL_model, gpu_check\n",
    "\n",
    "# gpu_check()\n",
    "\n",
    "data_file = \"./ML_replay_memories/replay_memory.csv\"  # Replace with your data file path\n",
    "output_model_path = \"./models\"\n",
    "input_dim = 173\n",
    "hidden_dim = 64\n",
    "train_DL_model(data_file, output_model_path, input_dim, hidden_dim, batch_size = 128, epochs = 25, lr = 0.004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data generation\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"./data_gen\")\n",
    "\n",
    "from data_generation import create_replay_memory_dataset\n",
    "import random\n",
    "from schnapsen.bots import RandBot\n",
    "from schnapsen.bots import RdeepBot\n",
    "from schnapsen.bots import BullyBot\n",
    "\n",
    "rng = random.Random(42)\n",
    "\n",
    "bot1 = RandBot(rng)\n",
    "bot2 = RdeepBot(num_samples=8, depth=4, rand=rng)\n",
    "bot4 = BullyBot(rng)\n",
    "\n",
    "create_replay_memory_dataset(bot1=bot2, bot2=bot2, num_of_games = 1, parallel = False, overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepCFR training\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"./models\")\n",
    "sys.path.append(\"./CFRmodels\")\n",
    "\n",
    "\n",
    "from DeepCFR import load_txt_dataset, create_data_loader, DeepCFR\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset from text file\n",
    "file_path = \"./ML_replay_memories/replay_memory.txt\"  # Replace with your dataset file\n",
    "features, regrets = load_txt_dataset(file_path, label_type=\"regret\")  # Load regret data\n",
    "_, strategies = load_txt_dataset(file_path, label_type=\"strategy\")   # Load strategy data\n",
    "\n",
    "# Verify dataset dimensions\n",
    "print(\"Features shape:\", features.shape)  # Should match number of rows and feature length\n",
    "print(\"Regrets shape:\", np.array(regrets).shape)  # Should match rows and action size\n",
    "print(\"Strategies shape:\", np.array(strategies).shape)  # Should match rows and action size\n",
    "\n",
    "# Create DataLoaders for training\n",
    "regret_loader = create_data_loader(features, regrets, batch_size=32, shuffle=True)\n",
    "strategy_loader = create_data_loader(features, strategies, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize the DeepCFR model\n",
    "deep_cfr = DeepCFR(input_size=features.shape[1], action_size=len(regrets[0]))\n",
    "\n",
    "\n",
    "# Train regret network\n",
    "print(\"Training regret network...\")\n",
    "deep_cfr.train_regret_network(regret_loader, epochs=10)\n",
    "\n",
    "# Train strategy network\n",
    "print(\"Training strategy network...\")\n",
    "deep_cfr.train_strategy_network(strategy_loader, epochs=10)\n",
    "\n",
    "torch.save(deep_cfr.regret_net.state_dict(), \"./CFRmodels/regret_net.pth\")\n",
    "torch.save(deep_cfr.strategy_net.state_dict(), \"./CFRmodels/strategy_net.pth\")\n",
    "\n",
    "print(\"Trained networks saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data-gen 2\n",
    "import sys\n",
    "sys.path.append(\"./data_gen\")\n",
    "from data_generation import MCTSbot\n",
    "from schnapsen.game import SchnapsenGamePlayEngine\n",
    "import random\n",
    "import pathlib\n",
    "\n",
    "def create_replay_memory_dataset_with_mcts(num_of_games: int = 1,\n",
    "                                           sims_per_move=100,\n",
    "                                           replay_memory_dir: str = \"MCTS_replay_memories\",\n",
    "                                           replay_memory_filename: str = \"replay_memory.csv\"):\n",
    "    engine = SchnapsenGamePlayEngine()\n",
    "    replay_memory_location = pathlib.Path(replay_memory_dir) / replay_memory_filename\n",
    "    replay_memory_location.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if replay_memory_location.exists():\n",
    "        replay_memory_location.unlink()\n",
    "\n",
    "    for game_id in range(1, num_of_games+1):\n",
    "        rng = random.Random(game_id)\n",
    "        bot1 = MCTSbot(replay_memory_file_path= replay_memory_location, num_samples= 2, depth= 4, rand= random.Random(game_id))\n",
    "        bot2 = MCTSbot(replay_memory_file_path= replay_memory_location, num_samples= 2, depth= 4, rand= random.Random(game_id))\n",
    "        engine.play_game(bot1, bot2, rng)\n",
    "        if game_id % 10 == 0:\n",
    "            print(f\"Game {game_id} complete\")\n",
    "\n",
    "create_replay_memory_dataset_with_mcts(num_of_games = 1, sims_per_move = 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== _apply_action DEBUG ===\n",
      "Perspective: Leader\n",
      "Leader hand: [Card.ACE_DIAMONDS, Card.ACE_SPADES, Card.ACE_CLUBS, Card.JACK_DIAMONDS, Card.QUEEN_CLUBS]\n",
      "Follower hand: [Card.JACK_CLUBS, Card.ACE_HEARTS, Card.TEN_SPADES, Card.TEN_DIAMONDS, Card.KING_SPADES]\n",
      "Chosen action: RegularMove(card=Card.ACE_DIAMONDS)\n",
      "=== _apply_action DEBUG ===\n",
      "Perspective: Follower\n",
      "Leader hand: [Card.ACE_DIAMONDS, Card.ACE_SPADES, Card.ACE_CLUBS, Card.JACK_DIAMONDS, Card.QUEEN_CLUBS]\n",
      "Follower hand: [Card.JACK_CLUBS, Card.ACE_HEARTS, Card.TEN_SPADES, Card.TEN_DIAMONDS, Card.KING_SPADES]\n",
      "Chosen action: RegularMove(card=Card.JACK_CLUBS)\n",
      "=== _apply_action DEBUG ===\n",
      "Perspective: Follower\n",
      "Leader hand: [Card.ACE_SPADES, Card.ACE_CLUBS, Card.JACK_DIAMONDS, Card.QUEEN_CLUBS, Card.KING_DIAMONDS]\n",
      "Follower hand: [Card.ACE_HEARTS, Card.TEN_SPADES, Card.TEN_DIAMONDS, Card.KING_SPADES, Card.JACK_SPADES]\n",
      "Chosen action: RegularMove(card=Card.ACE_HEARTS)\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Trying to remove a card from the hand which is not in the hand. Hand is [Card.ACE_SPADES, Card.ACE_CLUBS, Card.JACK_DIAMONDS, Card.QUEEN_CLUBS, Card.KING_DIAMONDS], trying to remove Card.JACK_CLUBS",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/workspace/uni_projects/cardbot/schnapsen/src/schnapsen/game.py:282\u001b[0m, in \u001b[0;36mHand.remove\u001b[0;34m(self, card)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcards\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcard\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n",
      "\u001b[0;31mValueError\u001b[0m: list.remove(x): x not in list",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# or run a tournament, etc.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 43\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 24\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m trainer \u001b[38;5;241m=\u001b[39m DeepCFRTrainer(engine\u001b[38;5;241m=\u001b[39mengine,\n\u001b[1;32m     18\u001b[0m                          trick_scorer\u001b[38;5;241m=\u001b[39mtrick_scorer,\n\u001b[1;32m     19\u001b[0m                          iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,        \u001b[38;5;66;03m# how many outer iterations\u001b[39;00m\n\u001b[1;32m     20\u001b[0m                          seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[1;32m     21\u001b[0m                          max_actions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 3. Run the training loop\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeals_per_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m              \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m              \u001b[49m\u001b[43mepochs_per_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# 4. Extract the trained model\u001b[39;00m\n\u001b[1;32m     29\u001b[0m regret_model \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mget_regret_model()\n",
      "File \u001b[0;32m~/workspace/uni_projects/cardbot/models/deepCFRv2.py:159\u001b[0m, in \u001b[0;36mDeepCFRTrainer.train\u001b[0;34m(self, deals_per_iteration, batch_size, epochs_per_iter)\u001b[0m\n\u001b[1;32m    157\u001b[0m     initial_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deal_random_initial_state()\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# Start recursion with active_player=0\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cfr_recursive\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgame_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mp0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mp1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactive_player\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleader_move\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# 2) Train the regret model with data in replay buffer\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs_per_iter):\n",
      "File \u001b[0;32m~/workspace/uni_projects/cardbot/models/deepCFRv2.py:256\u001b[0m, in \u001b[0;36mDeepCFRTrainer._cfr_recursive\u001b[0;34m(self, game_state, p0, p1, active_player, leader_move)\u001b[0m\n\u001b[1;32m    253\u001b[0m next_active_player \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (next_state\u001b[38;5;241m.\u001b[39mleader \u001b[38;5;129;01mis\u001b[39;00m game_state\u001b[38;5;241m.\u001b[39mleader) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m active_player \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 256\u001b[0m     child_utility \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cfr_recursive\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgame_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mp0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmy_p\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mp1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mother_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactive_player\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnext_active_player\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleader_move\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_regular_move\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     child_utility \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cfr_recursive(\n\u001b[1;32m    265\u001b[0m         game_state\u001b[38;5;241m=\u001b[39mnext_state,\n\u001b[1;32m    266\u001b[0m         p0\u001b[38;5;241m=\u001b[39mother_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    269\u001b[0m         leader_move\u001b[38;5;241m=\u001b[39ma \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39mis_regular_move() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    270\u001b[0m     )\n",
      "File \u001b[0;32m~/workspace/uni_projects/cardbot/models/deepCFRv2.py:264\u001b[0m, in \u001b[0;36mDeepCFRTrainer._cfr_recursive\u001b[0;34m(self, game_state, p0, p1, active_player, leader_move)\u001b[0m\n\u001b[1;32m    256\u001b[0m         child_utility \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cfr_recursive(\n\u001b[1;32m    257\u001b[0m             game_state\u001b[38;5;241m=\u001b[39mnext_state,\n\u001b[1;32m    258\u001b[0m             p0\u001b[38;5;241m=\u001b[39mmy_p \u001b[38;5;241m*\u001b[39m strategy[i],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    261\u001b[0m             leader_move\u001b[38;5;241m=\u001b[39ma \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39mis_regular_move() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    262\u001b[0m         )\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 264\u001b[0m         child_utility \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cfr_recursive\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgame_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m            \u001b[49m\u001b[43mp0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mother_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m            \u001b[49m\u001b[43mp1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmy_p\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m            \u001b[49m\u001b[43mactive_player\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnext_active_player\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m            \u001b[49m\u001b[43mleader_move\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_regular_move\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     node_utility[a] \u001b[38;5;241m=\u001b[39m child_utility\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# 8. Compute total node util\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/uni_projects/cardbot/models/deepCFRv2.py:250\u001b[0m, in \u001b[0;36mDeepCFRTrainer._cfr_recursive\u001b[0;34m(self, game_state, p0, p1, active_player, leader_move)\u001b[0m\n\u001b[1;32m    248\u001b[0m node_utility \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(actions):\n\u001b[0;32m--> 250\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgame_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperspective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleader_move\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;66;03m# who acts next\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;66;03m# for Schnapsen, usually the winner of the trick is next leader, so adapt as needed.\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     next_active_player \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (next_state\u001b[38;5;241m.\u001b[39mleader \u001b[38;5;129;01mis\u001b[39;00m game_state\u001b[38;5;241m.\u001b[39mleader) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/workspace/uni_projects/cardbot/models/deepCFRv2.py:324\u001b[0m, in \u001b[0;36mDeepCFRTrainer._apply_action\u001b[0;34m(self, old_state, action, perspective, leader_move)\u001b[0m\n\u001b[1;32m    322\u001b[0m follower_move \u001b[38;5;241m=\u001b[39m cast(RegularMove, action)  \u001b[38;5;66;03m# must be a regular move\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# Now apply both:\u001b[39;00m\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_leader_follower_moves\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mold_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleader_move\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollower_move\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/uni_projects/cardbot/models/deepCFRv2.py:602\u001b[0m, in \u001b[0;36mapply_leader_follower_moves\u001b[0;34m(engine, old_state, leader_move, follower_move)\u001b[0m\n\u001b[1;32m    600\u001b[0m     regular_leader_move: RegularMove \u001b[38;5;241m=\u001b[39m cast(RegularMove, leader_move)\n\u001b[1;32m    601\u001b[0m     leader_card \u001b[38;5;241m=\u001b[39m regular_leader_move\u001b[38;5;241m.\u001b[39mcard\n\u001b[0;32m--> 602\u001b[0m     \u001b[43mnext_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleader_card\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# 3) Remove the follower's card\u001b[39;00m\n\u001b[1;32m    605\u001b[0m next_state\u001b[38;5;241m.\u001b[39mfollower\u001b[38;5;241m.\u001b[39mhand\u001b[38;5;241m.\u001b[39mremove(follower_move\u001b[38;5;241m.\u001b[39mcard)\n",
      "File \u001b[0;32m~/workspace/uni_projects/cardbot/schnapsen/src/schnapsen/game.py:284\u001b[0m, in \u001b[0;36mHand.remove\u001b[0;34m(self, card)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcards\u001b[38;5;241m.\u001b[39mremove(card)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to remove a card from the hand which is not in the hand. Hand is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcards\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, trying to remove \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcard\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mve\u001b[39;00m\n",
      "\u001b[0;31mException\u001b[0m: Trying to remove a card from the hand which is not in the hand. Hand is [Card.ACE_SPADES, Card.ACE_CLUBS, Card.JACK_DIAMONDS, Card.QUEEN_CLUBS, Card.KING_DIAMONDS], trying to remove Card.JACK_CLUBS"
     ]
    }
   ],
   "source": [
    "# CFRtrainer\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"./models\")\n",
    "from deepCFRv2 import DeepCFRTrainer\n",
    "from schnapsen.game import SchnapsenGamePlayEngine, SchnapsenTrickScorer\n",
    "from schnapsen.bots import RandBot\n",
    "import random\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 1. Construct the game engine & trick scorer\n",
    "    engine = SchnapsenGamePlayEngine()\n",
    "    trick_scorer = SchnapsenTrickScorer()\n",
    "\n",
    "    # 2. Create the trainer\n",
    "    trainer = DeepCFRTrainer(engine=engine,\n",
    "                             trick_scorer=trick_scorer,\n",
    "                             iterations=100,        # how many outer iterations\n",
    "                             seed=42,\n",
    "                             max_actions=10)\n",
    "\n",
    "    # 3. Run the training loop\n",
    "    trainer.train(deals_per_iteration=10,\n",
    "                  batch_size=256,\n",
    "                  epochs_per_iter=1)\n",
    "\n",
    "    # 4. Extract the trained model\n",
    "    regret_model = trainer.get_regret_model()\n",
    "\n",
    "    # 5. Create a bot from the trained model\n",
    "    deep_cfr_bot = DeepCFRBot(regret_model=regret_model, max_actions=10)\n",
    "\n",
    "    # 6. Now, for example, you can pit this bot against a random bot, or a baseline bot:\n",
    "    rng = random.Random(42)\n",
    "    random_bot = RandBot(rng)\n",
    "\n",
    "    # 7. Let them play a few matches:\n",
    "    engine.play_game(deep_cfr_bot, random_bot)\n",
    "    # or run a tournament, etc.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
